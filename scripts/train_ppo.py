# scripts/train_ppo.py
from __future__ import annotations
import os, yaml, numpy as np, sys
from pathlib import Path
import multiprocessing as mp

# A√±adir el directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

# ‚Üê NUEVO: Configurar multiprocessing para Windows
if __name__ == "__main__":
    # Configurar el m√©todo de inicio de procesos para Windows
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        # Ya est√° configurado
        pass

from stable_baselines3 import PPO
from stable_baselines3.common.logger import configure
from train_env.vec_factory_chrono import make_vec_envs_chrono
from train_env.callbacks import PeriodicCheckpoint, StrategyKeeper, StrategyConsultant, AntiBadStrategy, MainModelSaver
from train_env.callbacks import TrainingMetricsCallback
from train_env.learning_rate_reset_callback import LearningRateResetCallback
from train_env.strategy_aggregator import aggregate_top_k
from train_env.model_manager import ModelManager
from base_env.config.config_loader import config_loader

def main():
    with open("config/train.yaml","r",encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    seed = int(cfg["seed"]); np.random.seed(seed)
    
    # Cargar s√≠mbolos desde symbols.yaml
    syms = config_loader.load_symbols()
    # ‚Üê NUEVO: Priorizar futuros para train_futures
    sym0 = next((s for s in syms if s.market == "futures"), None)
    if sym0 is None:
        sym0 = next((s for s in syms if s.market == "spot"), syms[0])
    symbol_cfg = {
        "symbol": sym0.symbol,
        "mode": f"train_{sym0.market}",
        "leverage": (sym0.leverage.__dict__ if sym0.leverage else None)
    }
    
    # Gestor de modelos por s√≠mbolo
    model_manager = ModelManager(
        symbol=sym0.symbol,
        models_root=cfg["models"]["root"],
        overwrite=cfg["models"]["overwrite"]
    )
    
    # Mostrar resumen del modelo
    model_manager.print_summary()
    
    # Obtener rutas de archivos
    file_paths = model_manager.get_file_paths()
    main_model_path = str(file_paths["model"])
    strat_prov = str(file_paths["provisional"])
    strat_best = str(file_paths["strategies"])
    strat_bad = str(file_paths["bad_strategies"])

    # ‚Üê NUEVO: Configurar verbosidad de logging
    train_verbosity = cfg["logging"].get("train_verbosity", "low")
    verbosity_levels = {
        "low": {"print_interval": 1000, "log_interval": 1000, "verbose": 0},
        "medium": {"print_interval": 100, "log_interval": 100, "verbose": 1},
        "high": {"print_interval": 1, "log_interval": 1, "verbose": 2}
    }
    
    verbosity_config = verbosity_levels.get(train_verbosity, verbosity_levels["low"])
    print(f"üìä Configuraci√≥n de logging: {train_verbosity} (intervalo: {verbosity_config['print_interval']} steps)")

    # Vec envs (cronol√≥gicos)
    venv = make_vec_envs_chrono(
        n_envs=cfg["env"]["n_envs"], seed=seed,
        data_cfg=cfg["data"], env_cfg=cfg["env"],
        logging_cfg=cfg["logging"], models_cfg=cfg["models"],
        symbol_cfg=symbol_cfg,              # <‚Äî aqu√≠ va el YAML del s√≠mbolo
        runs_log_cfg=cfg.get("runs_log", {})  # <‚Äî configuraci√≥n de retenci√≥n de runs
    )

    # Logger
    log_dir = cfg["log_dir"]; os.makedirs(log_dir, exist_ok=True)
    new_logger = configure(log_dir, ["stdout","tensorboard"])

    # PPO (crear o reanudar usando ModelManager)
    ppo_cfg = cfg["ppo"]
    
    # Configurar par√°metros de PPO
    policy_kwargs = ppo_cfg.get("policy_kwargs", {})
    if policy_kwargs:
        # Convertir activation_fn string a funci√≥n real
        if policy_kwargs.get("activation_fn") == "tanh":
            import torch.nn as nn
            policy_kwargs["activation_fn"] = nn.Tanh
    
    # ‚Üê NUEVO: Configurar learning rate annealing
    if ppo_cfg.get("anneal_lr", False):
        # Learning rate annealing: 3e-4 ‚Üí 1e-5
        import torch.optim as optim
        def lr_schedule(progress_remaining):
            return 1e-5 + (3e-4 - 1e-5) * progress_remaining
        ppo_cfg["learning_rate"] = lr_schedule
    
    # Separar total_timesteps del resto de la configuraci√≥n PPO
    total_timesteps = ppo_cfg.pop("total_timesteps", 50000000)
    
    # Filtrar par√°metros que no son v√°lidos para PPO
    ppo_valid_params = {
        'learning_rate', 'n_steps', 'batch_size', 'n_epochs', 'gamma', 'gae_lambda',
        'clip_range', 'clip_range_vf', 'ent_coef', 'vf_coef', 'max_grad_norm',
        'use_sde', 'sde_sample_freq', 'target_kl', 'tensorboard_log', 'policy_kwargs',
        'verbose', 'seed', 'device', 'stats_window_size', 'tensorboard_log'
    }
    
    # Crear diccionario solo con par√°metros v√°lidos para PPO
    ppo_kwargs = {k: v for k, v in ppo_cfg.items() if k in ppo_valid_params}
    
    # ‚Üê NUEVO: Aplicar verbosidad al modelo PPO
    ppo_kwargs["verbose"] = verbosity_config["verbose"]
    
    # Cargar o crear modelo usando ModelManager
    model = model_manager.load_model(
        env=venv,
        device="auto",
        **ppo_kwargs
    )
    model.set_logger(new_logger)

    # Callbacks
    ckpt_every = int(cfg["logging"]["checkpoint_every_steps"])
    callback_verbose = 1 if train_verbosity in ["medium", "high"] else 0
    
    callbacks = [
        PeriodicCheckpoint(save_every_steps=ckpt_every, save_path=os.path.join(log_dir,"checkpoints"), verbose=callback_verbose),
        StrategyKeeper(
            provisional_file=strat_prov,
            best_json_file=strat_best,
            top_k=int(cfg["logging"].get("top_k", 1000)),
            every_steps=ckpt_every,
            verbose=callback_verbose
        ),
        # ‚Üê NUEVO: Callback que consulta estrategias existentes para mejorar el aprendizaje
        StrategyConsultant(
            strategies_file=strat_best,
            consult_every_steps=ckpt_every,  # Consultar cada checkpoint
            verbose=callback_verbose
        ),
        # ‚Üê NUEVO: Callback que identifica y evita las PEORES estrategias
        AntiBadStrategy(
            strategies_file=strat_best,
            bad_strategies_file=strat_bad,
            consult_every_steps=ckpt_every,  # Consultar cada checkpoint
            verbose=callback_verbose
        ),
        # ‚Üê NUEVO: Callback para guardar modelo principal usando ModelManager
        MainModelSaver(save_every_steps=int(cfg["models"]["save_every_steps"]), fixed_path=main_model_path, model_manager=model_manager, verbose=callback_verbose),
    ]
    
    # ‚Üê NUEVO: A√±adir callback de m√©tricas de entrenamiento si est√° habilitado
    metrics_cfg = cfg.get("metrics", {})
    if metrics_cfg.get("enable", True):
        # Ajustar intervalo seg√∫n verbosidad
        base_interval = int(metrics_cfg.get("interval", 2048))
        if train_verbosity == "low":
            metrics_interval = max(base_interval, 1000)  # M√≠nimo 1000 steps
        elif train_verbosity == "medium":
            metrics_interval = max(base_interval, 100)   # M√≠nimo 100 steps
        else:  # high
            metrics_interval = base_interval              # Usar intervalo base
        
        metrics_path_pattern = metrics_cfg.get("path_pattern", "models/{symbol}/{symbol}_train_metrics.jsonl")
        metrics_path = metrics_path_pattern.format(symbol=symbol_cfg["symbol"])
        
        metrics_callback = TrainingMetricsCallback(
            symbol=symbol_cfg["symbol"],
            mode=symbol_cfg["mode"],
            metrics_path=metrics_path,
            log_interval=metrics_interval,
            verbose=callback_verbose
        )
        callbacks.append(metrics_callback)
        print(f"üìä Callback de m√©tricas habilitado: {metrics_path} (intervalo: {metrics_interval})")
    else:
        print("üìä Callback de m√©tricas deshabilitado")

    # ‚Üê NUEVO: Sistema anti-congelamiento (mensajes seg√∫n YAML)
    print(f"üõ°Ô∏è Sistema anti-congelamiento activado:")
    print(f"   - Entropy coefficient: {ppo_cfg['ent_coef']}")
    print(f"   - Learning rate annealing: {'Activado' if ppo_cfg.get('anneal_lr', False) else 'Desactivado'}")
    print(f"   - Target KL: {ppo_cfg.get('target_kl', 'No configurado')}")
    lr_reset_cfg = ppo_cfg.get('lr_reset', {"enabled": False})
    if lr_reset_cfg.get('enabled', False):
        print(f"   - Learning rate reset: {lr_reset_cfg.get('threshold_runs', 30)} runs vac√≠os ‚Üí LR variable (1e-4 a 1e-2)")
        # A√±adir el callback de reset de learning rate
        callbacks.append(LearningRateResetCallback(
            env=venv,
            reset_threshold=lr_reset_cfg.get('threshold_runs', 30),
            verbose=1
        ))
    else:
        print(f"   - Learning rate reset: Desactivado")
    
    # Entrenar con manejo de errores robusto
    try:
        print(f"üöÄ INICIANDO ENTRENAMIENTO PPO - {total_timesteps:,} steps")
        print(f"üìä Configuraci√≥n: {ppo_cfg['n_steps']} steps/env, {ppo_cfg['batch_size']} batch, {ppo_cfg['learning_rate']} lr")
        print(f"üîß Multiprocessing: {cfg['env']['n_envs']} workers con m√©todo 'spawn'")
        print(f"üìù Logging: {train_verbosity} (verbose={verbosity_config['verbose']}, intervalo={verbosity_config['print_interval']} steps)")
        
        # ‚Üê NUEVO: Configurar variables de entorno para estabilidad
        os.environ['PYTHONHASHSEED'] = str(seed)
        os.environ['OMP_NUM_THREADS'] = '1'  # Evitar conflictos de threading
        
        model.learn(total_timesteps=int(total_timesteps), callback=callbacks)
        
        print("‚úÖ Entrenamiento completado exitosamente")
        
        # Guardado final en la ruta fija (sobrescribe)
        model.save(main_model_path)
        print(f"[MODEL] Final PPO saved -> {main_model_path}")

        # √öltima consolidaci√≥n de estrategias
        aggregate_top_k(strat_prov, strat_best, int(cfg["logging"].get("top_k", 1000)))
        print(f"[STRAT] Best strategies -> {strat_best}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Entrenamiento interrumpido por el usuario")
        print("üíæ Guardando modelo en estado actual...")
        model.save(main_model_path)
        print(f"[MODEL] Modelo guardado en estado interrumpido -> {main_model_path}")
        
    except (EOFError, BrokenPipeError) as e:
        print(f"\n‚ùå ERROR DE MULTIPROCESSING: {type(e).__name__}: {e}")
        print("üîß Esto puede ser causado por:")
        print("   - Demasiados workers para los recursos disponibles")
        print("   - Problemas de comunicaci√≥n entre procesos")
        print("   - Memoria insuficiente")
        print("üíæ Intentando guardar modelo en estado de error...")
        try:
            model.save(main_model_path)
            print(f"[MODEL] Modelo guardado en estado de error -> {main_model_path}")
        except Exception as save_error:
            print(f"‚ùå No se pudo guardar el modelo: {save_error}")
        
        # No re-raise para evitar crash completo
        print("‚ö†Ô∏è Continuando sin re-lanzar el error...")
        
    except Exception as e:
        print(f"\n‚ùå ERROR durante el entrenamiento: {type(e).__name__}: {e}")
        print("üíæ Intentando guardar modelo en estado de error...")
        try:
            model.save(main_model_path)
            print(f"[MODEL] Modelo guardado en estado de error -> {main_model_path}")
        except Exception as save_error:
            print(f"‚ùå No se pudo guardar el modelo: {save_error}")
        
        # Re-raise para debugging
        raise

if __name__ == "__main__":
    main()
