# üéØ Entorno de Entrenamiento (Training Environment)

Este directorio contiene todos los componentes necesarios para convertir el entorno base de trading en un entorno compatible con Gymnasium y Stable-Baselines3 para entrenamiento de Reinforcement Learning (RL).

## üóÇÔ∏è Estructura de Archivos

### üéÆ **gym_wrapper.py** - Wrapper de Gymnasium
Convierte el `BaseTradingEnv` en un entorno compatible con Gymnasium.

**Caracter√≠sticas principales:**
- **Action Space**: 5 acciones discretas (0-4)
- **Observation Space**: Vector de features aplanado
- **Reward Shaping**: Integraci√≥n con `RewardShaper`
- **Strategy Logging**: Registro de eventos de trading

**Acciones disponibles:**
- `0`: Dejar que la policy decida (sin override)
- `1`: Cerrar todas las posiciones
- `2`: Bloquear aperturas de nuevas posiciones
- `3`: Forzar apertura LONG con SL/TP autom√°ticos
- `4`: Forzar apertura SHORT con SL/TP autom√°ticos

**Observaci√≥n aplanada:**
```python
def _obs_dim(self) -> int:
    per_tf = 7      # 7 features por timeframe
    pos = 4         # 4 features de posici√≥n
    ana = 2         # 2 features de an√°lisis
    return len(self.tfs)*per_tf + pos + ana
```

**Features por timeframe:**
1. `close` - Precio de cierre
2. `ema20` - Media m√≥vil exponencial 20
3. `ema50` - Media m√≥vil exponencial 50
4. `rsi14` - RSI 14 per√≠odos
5. `atr14` - ATR 14 per√≠odos
6. `macd_hist` - Histograma MACD
7. `bb_p` - Posici√≥n en Bandas de Bollinger

**Features de posici√≥n:**
1. `side` - Lado de la posici√≥n (0, +1, -1)
2. `qty` - Cantidad de la posici√≥n
3. `entry_price` - Precio de entrada
4. `unrealized_pnl` - PnL no realizado

**Features de an√°lisis:**
1. `confidence` - Nivel de confianza de la estrategia
2. `side_hint` - Sugerencia de direcci√≥n

### üéÅ **reward_shaper.py** - Moldeador de Rewards
Sistema avanzado de c√°lculo de rewards basado en m√∫ltiples factores.

**Componentes del reward:**

#### üèÜ **Tiers por ROI (Tramos)**
```yaml
tiers:
  pos: [[0, 1, 0.1], [1, 3, 0.5], [3, 10, 1.0], [10, 100, 2.0]]
  neg: [[0, 1, -0.1], [1, 3, -0.5], [3, 10, -1.0], [10, 100, -2.0]]
```
- Rewards escalonados seg√∫n el porcentaje de ROI
- Diferentes escalas para ganancias y p√©rdidas
- Formato: `[min_roi%, max_roi%, reward_value]`

#### üéØ **Bonuses por TP/SL**
```yaml
bonuses:
  tp_hit: 1.0      # Bonus por alcanzar take profit
  sl_hit: -0.5     # Penalizaci√≥n por alcanzar stop loss
```

#### ‚öñÔ∏è **Pesos de componentes**
```yaml
weights:
  realized_pnl: 1.0        # PnL realizado (del entorno)
  unrealized_pnl: 0.1      # PnL no realizado (gu√≠a suave)
  r_multiple: 0.5          # R-multiple del trade
  risk_efficiency: 0.3     # Eficiencia de riesgo
  time_penalty: -0.01      # Penalizaci√≥n por tiempo en posici√≥n
  trade_cost: -0.1         # Coste por operaci√≥n
  dd_penalty: -0.2         # Penalizaci√≥n por drawdown
```

**C√°lculo de reward:**
```python
def compute(self, obs, base_reward, events):
    # Componentes continuos
    reward = (self.w_realized * realized_usd + 
              self.w_unreal * unreal_usd + 
              self.w_time * time_penalty + 
              self.w_dd * drawdown_penalty)
    
    # Eventos de cierre
    if close_event:
        roi_pct = close_event.get("roi_pct", 0.0)
        r_mult = close_event.get("r_multiple", 0.0)
        risk_pct = close_event.get("risk_pct", 0.0)
        
        # Reward por ROI seg√∫n tiers
        if roi_pct >= 0:
            reward += self._tier_value(self.tiers_pos, roi_pct)
        else:
            reward += self._tier_value(self.tiers_neg, abs(roi_pct))
        
        # Bonus por TP/SL
        if tp_hit: reward += self.bon_tp
        if sl_hit: reward += self.bon_sl
        
        # Refuerzos por calidad
        reward += self.w_rmult * r_mult
        if risk_pct > 0:
            risk_eff = abs(roi_pct) / risk_pct
            reward += self.w_risk_eff * risk_eff
    
    return self._clip(reward)
```

### üìä **strategy_logger.py** - Logger de Estrategia
Registra todos los eventos de trading para an√°lisis posterior.

**Funcionalidades:**
- **Append Events**: A√±ade m√∫ltiples eventos al log
- **File Management**: Manejo autom√°tico de archivos de log
- **Event Tracking**: Seguimiento de OPEN, CLOSE, TP_HIT, SL_HIT

### üîÑ **vec_factory.py** - Factory de Entornos Vectorizados
Crea entornos vectorizados para entrenamiento paralelo.

**Caracter√≠sticas:**
- **Multi-Environment**: M√∫ltiples entornos paralelos
- **Chronological**: Opci√≥n de datos cronol√≥gicos vs aleatorios
- **Warmup**: Barras de calentamiento antes del entrenamiento

### ‚è∞ **vec_factory_chrono.py** - Factory Cronol√≥gica
Versi√≥n especializada para entrenamiento con datos cronol√≥gicos.

**Ventajas:**
- **Realistic Training**: Entrenamiento m√°s realista
- **Temporal Consistency**: Consistencia temporal entre entornos
- **Market Conditions**: Simula condiciones de mercado reales

### üìà **dataset.py** - Gesti√≥n de Datos
Maneja la carga y preparaci√≥n de datos para entrenamiento.

**Funcionalidades:**
- **Data Loading**: Carga de datos hist√≥ricos
- **Timeframe Alignment**: Alineaci√≥n de m√∫ltiples timeframes
- **Feature Computation**: C√°lculo de indicadores t√©cnicos
- **Data Validation**: Validaci√≥n de integridad de datos

### üìù **callbacks.py** - Callbacks de Entrenamiento
Callbacks personalizados para monitoreo y control del entrenamiento.

**Callbacks incluidos:**
- **Checkpoint Callback**: Guardado autom√°tico de modelos
- **Logging Callback**: Logging de m√©tricas de entrenamiento
- **Early Stopping**: Parada temprana basada en criterios
- **Custom Metrics**: M√©tricas personalizadas de trading

### üéØ **strategy_aggregator.py** - Agregador de Estrategias
Combina m√∫ltiples estrategias o modelos para mejor performance.

**Funcionalidades:**
- **Ensemble Methods**: M√©todos de ensemble para m√∫ltiples modelos
- **Strategy Selection**: Selecci√≥n din√°mica de estrategias
- **Performance Tracking**: Seguimiento de performance por estrategia
- **Risk Management**: Gesti√≥n de riesgo agregada

## üöÄ **Uso del Sistema**

### 1. **Configuraci√≥n B√°sica**
```python
from train_env.gym_wrapper import TradingGymWrapper
from train_env.reward_shaper import RewardShaper
from base_env.base_env import BaseTradingEnv

# Crear entorno base
base_env = BaseTradingEnv(cfg, broker, oms)

# Crear wrapper de gym
gym_env = TradingGymWrapper(
    base_env=base_env,
    reward_yaml="config/rewards.yaml",
    tfs=["1m", "5m"],
    strategy_log_path="logs/strategy.log"
)
```

### 2. **Entrenamiento con PPO**
```python
from stable_baselines3 import PPO
from train_env.vec_factory import create_vec_env

# Crear entorno vectorizado
vec_env = create_vec_env(
    env_class=TradingGymWrapper,
    n_envs=4,
    env_kwargs={...}
)

# Entrenar modelo
model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=15000000)
```

### 3. **Configuraci√≥n de Rewards**
```yaml
# config/rewards.yaml
tiers:
  pos: [[0, 1, 0.1], [1, 3, 0.5], [3, 10, 1.0]]
  neg: [[0, 1, -0.1], [1, 3, -0.5], [3, 10, -1.0]]

weights:
  realized_pnl: 1.0
  r_multiple: 0.5
  risk_efficiency: 0.3
```

## üîß **Personalizaci√≥n**

### **Modificar Rewards**
1. Ajusta los `tiers` para diferentes niveles de ROI
2. Modifica los `weights` para cambiar la importancia de cada componente
3. A√±ade nuevos componentes de reward en `reward_shaper.py`

### **Cambiar Features**
1. Modifica `_obs_dim()` en `gym_wrapper.py`
2. Ajusta `_flatten_obs()` para incluir nuevos indicadores
3. Actualiza la configuraci√≥n en `pipeline.yaml`

### **A√±adir Callbacks**
1. Crea nuevos callbacks en `callbacks.py`
2. Implementa l√≥gica personalizada de monitoreo
3. Integra con el sistema de logging existente

## üìä **Monitoreo y Logging**

### **Logs de Estrategia**
- **Eventos de Trading**: OPEN, CLOSE, TP_HIT, SL_HIT
- **M√©tricas de Performance**: ROI%, R-multiple, Risk%
- **An√°lisis de Riesgo**: Drawdown, exposici√≥n, eficiencia

### **M√©tricas de Entrenamiento**
- **Reward Components**: Desglose de cada componente del reward
- **Trading Metrics**: Estad√≠sticas de trading por episodio
- **Risk Metrics**: M√©tricas de riesgo y gesti√≥n

### **TensorBoard Integration**
- **Training Curves**: Curvas de entrenamiento en tiempo real
- **Custom Metrics**: M√©tricas personalizadas de trading
- **Hyperparameter Tracking**: Seguimiento de hiperpar√°metros

## üéØ **Mejores Pr√°cticas**

### **Configuraci√≥n de Rewards**
1. **Balance**: Equilibra rewards positivos y negativos
2. **Escalado**: Usa escalas apropiadas para cada componente
3. **Consistencia**: Mant√©n consistencia entre diferentes timeframes

### **Entrenamiento**
1. **Warmup**: Usa suficientes barras de calentamiento
2. **Validation**: Valida en datos no vistos durante entrenamiento
3. **Monitoring**: Monitorea m√©tricas de trading, no solo reward

### **Risk Management**
1. **Position Sizing**: Ajusta tama√±os de posici√≥n seg√∫n el modelo
2. **Stop Losses**: Usa SL din√°micos basados en ATR
3. **Drawdown Limits**: Establece l√≠mites claros de drawdown

## üöÄ **Pr√≥ximos Pasos**

1. **Optimizar Rewards**: Ajustar funci√≥n de reward para mejor convergencia
2. **Feature Engineering**: A√±adir indicadores t√©cnicos m√°s sofisticados
3. **Ensemble Methods**: Implementar m√©todos de ensemble para m√∫ltiples modelos
4. **Live Trading**: Preparar para transici√≥n a trading en vivo
5. **Performance Analysis**: An√°lisis profundo de m√©tricas de trading
