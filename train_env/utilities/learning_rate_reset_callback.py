# train_env/learning_rate_reset_callback.py
# DescripciÃ³n: Callback para reset automÃ¡tico del learning rate cuando hay runs vacÃ­os consecutivos

from typing import Any, Dict, List, Optional, Union
import numpy as np
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import VecEnv


class LearningRateResetCallback(BaseCallback):
    """
    Callback que detecta runs vacÃ­os consecutivos y resetea el learning rate
    para forzar al agente a explorar nuevas estrategias.
    """
    
    def __init__(
        self,
        env: VecEnv,
        reset_threshold: int = 30,  # â† NUEVO: Umbral mÃ¡s bajo para activaciÃ³n mÃ¡s temprana
        new_learning_rate: float = 1e-3,
        verbose: int = 0
    ):
        super().__init__(verbose)
        self.env = env
        self.reset_threshold = reset_threshold
        self.new_learning_rate = new_learning_rate
        self.last_reset_step = 0
        self.reset_count = 0
        self.cooldown_steps = 5000  # â† NUEVO: Cooldown mÃ¡s largo para evitar resets excesivos
        self.learning_rates = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 5e-3]  # â† NUEVO: MÃ¡s opciones de LR
        self.entropy_boost = 0.2  # â† NUEVO: Boost temporal de entropÃ­a
        self.entropy_decay_steps = 10000  # â† NUEVO: Pasos para reducir entropÃ­a gradualmente
        self.original_ent_coef = None  # â† NUEVO: EntropÃ­a original para restaurar
        
    def _on_step(self) -> bool:
        """Se ejecuta en cada step del entrenamiento"""
        
        # Verificar si algÃºn entorno necesita reset de learning rate
        needs_reset = False
        
        # Acceder a los entornos de manera compatible con SubprocVecEnv
        try:
            # Para SubprocVecEnv, usar get_attr para acceder a mÃ©todos
            if hasattr(self.env, 'get_attr'):
                reset_flags = self.env.get_attr('needs_learning_rate_reset')
                if any(reset_flags):
                    needs_reset = True
            else:
                # Fallback para otros tipos de VecEnv
                for i in range(self.env.num_envs):
                    if hasattr(self.env.envs[i], 'needs_learning_rate_reset'):
                        if self.env.envs[i].needs_learning_rate_reset():
                            needs_reset = True
                            break
        except Exception as e:
            if self.verbose > 0:
                print(f"âš ï¸ Error accediendo a entornos: {e}")
            return True
        
        if needs_reset:
            # â† NUEVO: Verificar cooldown para evitar resets excesivos
            steps_since_last_reset = self.num_timesteps - self.last_reset_step
            if steps_since_last_reset < self.cooldown_steps:
                if self.verbose > 0:
                    print(f"â³ Cooldown activo: {steps_since_last_reset}/{self.cooldown_steps} steps")
                return True
            
            # â† NUEVO: Seleccionar learning rate diferente y boost de entropÃ­a
            old_lr = self.model.learning_rate
            old_ent_coef = self.model.ent_coef
            
            # Guardar entropÃ­a original si es la primera vez
            if self.original_ent_coef is None:
                self.original_ent_coef = old_ent_coef
            
            new_lr = self.learning_rates[self.reset_count % len(self.learning_rates)]
            new_ent_coef = old_ent_coef + self.entropy_boost  # Boost temporal de entropÃ­a
            
            self.model.learning_rate = new_lr
            self.model.ent_coef = new_ent_coef
            
            # Reset de flags en todos los entornos
            try:
                if hasattr(self.env, 'get_attr'):
                    # Para SubprocVecEnv
                    self.env.env_method('reset_learning_rate_flag')
                else:
                    # Fallback para otros tipos
                    for i in range(self.env.num_envs):
                        if hasattr(self.env.envs[i], 'reset_learning_rate_flag'):
                            self.env.envs[i].reset_learning_rate_flag()
            except Exception as e:
                if self.verbose > 0:
                    print(f"âš ï¸ Error reseteando flags: {e}")
            
            # Logging
            self.reset_count += 1
            self.last_reset_step = self.num_timesteps
            
            if self.verbose > 0:
                print(f"ğŸ”„ LEARNING RATE RESET #{self.reset_count}")
                print(f"   Step: {self.num_timesteps}")
                print(f"   LR anterior: {old_lr:.2e} â†’ LR nuevo: {new_lr:.2e}")
                print(f"   EntropÃ­a anterior: {old_ent_coef:.3f} â†’ EntropÃ­a nueva: {new_ent_coef:.3f}")
                print(f"   Motivo: {self.reset_threshold} runs vacÃ­os consecutivos")
                print(f"   Cooldown: {self.cooldown_steps} steps")
                print(f"   El agente ahora explorarÃ¡ nuevas estrategias con mayor entropÃ­a")
        
        # â† NUEVO: Reducir entropÃ­a gradualmente despuÃ©s del boost
        if self.original_ent_coef is not None and self.model.ent_coef > self.original_ent_coef:
            steps_since_boost = self.num_timesteps - self.last_reset_step
            if steps_since_boost > 0 and steps_since_boost <= self.entropy_decay_steps:
                # Reducir entropÃ­a gradualmente
                decay_factor = 1.0 - (steps_since_boost / self.entropy_decay_steps)
                target_ent_coef = self.original_ent_coef + (self.entropy_boost * decay_factor)
                self.model.ent_coef = max(target_ent_coef, self.original_ent_coef)
        
        return True
    
    def _on_training_end(self) -> None:
        """Se ejecuta al final del entrenamiento"""
        if self.verbose > 0:
            print(f"ğŸ“Š Resumen de Learning Rate Resets:")
            print(f"   Total resets: {self.reset_count}")
            print(f"   Ãšltimo reset en step: {self.last_reset_step}")
            print(f"   LR final: {self.model.learning_rate:.2e}")
